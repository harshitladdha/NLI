{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#DROPOUT + L2-REGULARIZATION\n",
    "from __future__ import print_function\n",
    "from functools import reduce\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import tarfile\n",
    "import tempfile\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(1337)  \n",
    "\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers import merge, recurrent, Dense, Input, Dropout, TimeDistributed\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.recurrent import LSTM, GRU, SimpleRNN\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.regularizers import l2\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def extract_tokens_from_binary_parse(parse):\n",
    "    return parse.replace('(', ' ').replace(')', ' ').replace('-LRB-', '(').replace('-RRB-', ')').split()\n",
    "\n",
    "def yield_examples(fn, skip_no_majority=True, limit=None):\n",
    "  for i, line in enumerate(open(fn)):\n",
    "    if limit and i > limit:\n",
    "      break\n",
    "    data = json.loads(line)\n",
    "    label = data['gold_label']\n",
    "    s1 = bytes(' '.join(extract_tokens_from_binary_parse(data['sentence1_binary_parse'])))\n",
    "    s2 = bytes(' '.join(extract_tokens_from_binary_parse(data['sentence2_binary_parse'])))\n",
    "    if skip_no_majority and label == '-':\n",
    "      continue\n",
    "    yield (label, s1, s2)\n",
    "\n",
    "def get_data(fn, limit=None):\n",
    "  raw_data = list(yield_examples(fn=fn, limit=limit))\n",
    "  left = [s1 for _, s1, s2 in raw_data]\n",
    "  right = [s2 for _, s1, s2 in raw_data]\n",
    "  print(max(len(x.split()) for x in left))\n",
    "  print(max(len(x.split()) for x in right))\n",
    "\n",
    "  LABELS = {'contradiction': 0, 'neutral': 1, 'entailment': 2}\n",
    "  Y = np.array([LABELS[l] for l, s1, s2 in raw_data])\n",
    "  Y = np_utils.to_categorical(Y, len(LABELS))\n",
    "\n",
    "  return left, right, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82\n",
      "62\n",
      "59\n",
      "55\n",
      "57\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "training = get_data('data/snli_1.0_train.jsonl')\n",
    "validation = get_data('data/snli_1.0_dev.jsonl')\n",
    "test = get_data('data/snli_1.0_test.jsonl')\n",
    "\n",
    "tokenizer = Tokenizer(lower=False, filters='')\n",
    "tokenizer.fit_on_texts(training[0] + training[1])\n",
    "VOCAB = len(tokenizer.word_counts) + 1\n",
    "LABELS = {'contradiction': 0, 'neutral': 1, 'entailment': 2}\n",
    "\n",
    "\n",
    "USE_GLOVE = True\n",
    "TRAIN_EMBED = False\n",
    "EMBED_HIDDEN_SIZE = 300\n",
    "MAX_LEN = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size = 42391\n"
     ]
    }
   ],
   "source": [
    "to_seq = lambda X: pad_sequences(tokenizer.texts_to_sequences(X), maxlen=MAX_LEN)\n",
    "prepare_data = lambda data: (to_seq(data[0]), to_seq(data[1]), data[2])\n",
    "\n",
    "training = prepare_data(training)\n",
    "validation = prepare_data(validation)\n",
    "test = prepare_data(test)\n",
    "\n",
    "print('Vocab size =', VOCAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GloVe\n",
      "Total number of null word embeddings:\n",
      "4043\n"
     ]
    }
   ],
   "source": [
    "GLOVE_STORE = 'data/precomputed_glove.weights'\n",
    "if USE_GLOVE:\n",
    "  if not os.path.exists(GLOVE_STORE):\n",
    "    print('Computing GloVe')\n",
    "  \n",
    "    embeddings_index = {}\n",
    "    f = open('glove.840B.300d.txt')\n",
    "    for line in f:\n",
    "      values = line.split()\n",
    "      word = values[0]\n",
    "      coefs = np.asarray(values[1:], dtype='float32')\n",
    "      embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    \n",
    "    embedding_matrix = np.zeros((VOCAB, EMBED_HIDDEN_SIZE))\n",
    "    for word, i in tokenizer.word_index.iteritems():\n",
    "      embedding_vector = embeddings_index.get(word)\n",
    "      if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "      else:\n",
    "        print('Missing from GloVe: {}'.format(word))\n",
    "  \n",
    "    np.save(open(GLOVE_STORE, 'w'), embedding_matrix)\n",
    "\n",
    "  print('Loading GloVe')\n",
    "  embedding_matrix = np.load(open(GLOVE_STORE))\n",
    "\n",
    "  print('Total number of null word embeddings:')\n",
    "  print(np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "\n",
    "  embed = Embedding(VOCAB, EMBED_HIDDEN_SIZE, weights=[embedding_matrix], input_length=MAX_LEN, trainable=TRAIN_EMBED)\n",
    "else:\n",
    "  embed = Embedding(VOCAB, EMBED_HIDDEN_SIZE, input_length=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(549367, 42) (549367, 42) (549367, 3) (9842, 42) (9842, 42) (9842, 3)\n",
      "(9824, 42) (9824, 42) (9824, 3)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.contrib import rnn\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "\n",
    "input_vec_size = 300\n",
    "lstm_size = 300\n",
    "time_step_size = 42\n",
    "\n",
    "\n",
    "\n",
    "trXP, trXH, trY, teXP, teXH, teY = training[0], training[1], training[2], validation[0], validation[1], validation[2]\n",
    "teXP1, teXH1, teY1 = test[0], test[1], test[2]\n",
    "\n",
    "print(trXP.shape, trXH.shape, trY.shape, teXP.shape, teXH.shape, teY.shape)\n",
    "print(teXP1.shape, teXH1.shape, teY1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def init_weights(shape):\n",
    "    return tf.Variable(tf.random_normal(shape, stddev=0.01))\n",
    "\n",
    "def model(X_P, X_H, lstm_size, dropout):\n",
    "    # X, input shape: (batch_size, time_step_size, input_vec_size)\n",
    "    print(\"X_P.get_shape\")\n",
    "    print(X_P.get_shape)\n",
    "    XT_P = tf.transpose(X_P, [1, 0, 2])  # permute time_step_size and batch_size\n",
    "    print(\"XT_P.get_shape\")\n",
    "    print(XT_P.get_shape)\n",
    "    # XT shape: (time_step_size, batch_size, input_vec_size)\n",
    "    XR_P = tf.reshape(XT_P, [-1, input_vec_size])\n",
    "    print(\"XR_P.get_shape\")\n",
    "    print(XR_P.get_shape)\n",
    "    # XR shape: (time_step_size * batch_size, input_vec_size)\n",
    "    X_split_P = tf.split(XR_P, time_step_size, 0) # split them to time_step_size (28 arrays)\n",
    "    XT_H = tf.transpose(X_H, [1, 0, 2])  # permute time_step_size and batch_size\n",
    "    XR_H = tf.reshape(XT_H, [-1, input_vec_size])\n",
    "    X_split_H = tf.split(XR_H, time_step_size, 0) # split them to time_step_size (28 arrays)\n",
    "\n",
    "    with tf.variable_scope('premise'):\n",
    "        lstm1 = tf.contrib.rnn.BasicLSTMCell(lstm_size, forget_bias=1.0, state_is_tuple=True)\n",
    "        cell1 = tf.contrib.rnn.DropoutWrapper(lstm1, output_keep_prob=dropout)\n",
    "        outputs_P, _states_P =  tf.contrib.rnn.static_rnn(cell1, X_split_P, dtype=tf.float32)\n",
    "        #outputs_P_batch_norm = tf.contrib.layers.batch_norm(outputs_P, center=True, scale=True, is_training=phase, scope='bn')\n",
    "\n",
    "    with tf.variable_scope('hypothesis'):\n",
    "        lstm2 = tf.contrib.rnn.BasicLSTMCell(lstm_size, forget_bias=1.0,  state_is_tuple=True)\n",
    "        cell2 = tf.contrib.rnn.DropoutWrapper(lstm2, output_keep_prob=dropout)\n",
    "        outputs_H, _states_H =  tf.contrib.rnn.static_rnn(cell2, X_split_H, dtype=tf.float32)\n",
    "        #outputs_H_batch_norm = tf.contrib.layers.batch_norm(outputs_H, center=True, scale=True, is_training=phase, scope='bn')\n",
    "\n",
    "    # Linear activation\n",
    "    # Get the last output\n",
    "    #l0 = tf.concat([outputs_P_batch_norm[-1], outputs_H_batch_norm[-1]] ,1 )\n",
    "    l0 = tf.concat([outputs_P[-1], outputs_H[-1]] ,1 )\n",
    "    l0_drop = tf.nn.dropout(l0, dropout)\n",
    "    #print(\"l0.get_shape()\", l0.get_shape())\n",
    "\n",
    "    W_1 = tf.Variable(tf.truncated_normal([lstm_size*2, lstm_size*2], stddev=1.0 / math.sqrt(lstm_size*2)))\n",
    "    B_1 = tf.Variable(tf.zeros([lstm_size*2]))\n",
    "    W_2 = tf.Variable(tf.truncated_normal([lstm_size*2, lstm_size*2], stddev=1.0 / math.sqrt(lstm_size*2)))\n",
    "    B_2 = tf.Variable(tf.zeros([lstm_size*2]))\n",
    "    W_3 = tf.Variable(tf.truncated_normal([lstm_size*2, lstm_size*2], stddev=1.0 / math.sqrt(lstm_size*2)))\n",
    "    B_3 = tf.Variable(tf.zeros([lstm_size*2]))\n",
    "    W_4 = tf.Variable(tf.truncated_normal([lstm_size*2, 3], stddev=1.0 / math.sqrt(3)))\n",
    "    B_4 = tf.Variable(tf.zeros([3]))\n",
    "\n",
    "    l1 = tf.nn.relu(tf.matmul(l0_drop, W_1) + B_1)\n",
    "    l1_drop = tf.nn.dropout(l1, dropout)\n",
    "    #l1_batch_norm = tf.contrib.layers.batch_norm(l1_drop, center=True, scale=True, is_training=phase, scope='bn')\n",
    "\n",
    "    l2 = tf.nn.relu(tf.matmul(l1_drop, W_2) + B_2)\n",
    "    l2_drop = tf.nn.dropout(l2, dropout)\n",
    "    #l2_batch_norm = tf.contrib.layers.batch_norm(l2_drop, center=True, scale=True, is_training=phase, scope='bn')\n",
    "\n",
    "    l3 = tf.nn.relu(tf.matmul(l2_drop, W_3) + B_3)\n",
    "    l3_drop = tf.nn.dropout(l3, dropout)\n",
    "    #l3_batch_norm = tf.contrib.layers.batch_norm(l3_drop, center=True, scale=True, is_training=phase, scope='bn')\n",
    "\n",
    "    l4 = tf.matmul(l3_drop, W_4) + B_4\n",
    "    regularizer = tf.nn.l2_loss(W_1) + tf.nn.l2_loss(W_2) + tf.nn.l2_loss(W_3)\n",
    "    print(\"regularizer.get_shape() :\", regularizer.get_shape())\n",
    "    return l4, regularizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_P.get_shape\n",
      "<bound method Tensor.get_shape of <tf.Tensor 'ToFloat:0' shape=(?, 42, 300) dtype=float32>>\n",
      "XT_P.get_shape\n",
      "<bound method Tensor.get_shape of <tf.Tensor 'transpose:0' shape=(42, ?, 300) dtype=float32>>\n",
      "XR_P.get_shape\n",
      "<bound method Tensor.get_shape of <tf.Tensor 'Reshape:0' shape=(?, 300) dtype=float32>>\n",
      "regularizer.get_shape() : ()\n"
     ]
    }
   ],
   "source": [
    "XP = tf.placeholder(\"int32\", [None, 42])\n",
    "XH = tf.placeholder(\"int32\", [None, 42])\n",
    "Y = tf.placeholder(\"int32\", [None, 3])\n",
    "dropout = tf.placeholder(tf.float32)\n",
    "phase = tf.placeholder(tf.bool, name='phase')\n",
    "beta = 4e-6\n",
    "\n",
    "import tensorflow as tf\n",
    "inputsH = tf.nn.embedding_lookup(embedding_matrix, XP)\n",
    "inputsP = tf.nn.embedding_lookup(embedding_matrix, XH)\n",
    "\n",
    "X1 = inputsH\n",
    "X_P = tf.to_float(X1, name='ToFloat')\n",
    "\n",
    "X2 = inputsP\n",
    "X_H = tf.to_float(X2, name='ToFloat')\n",
    "\n",
    "py_x, regularizer = model(X_P, X_H, lstm_size, dropout)\n",
    "#py_x = model(X_P, X_H, lstm_size, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost1.get_shape() : ()\n",
      "regularizer.get_shape() : ()\n",
      "cost.get_shape() : ()\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope('cost'):\n",
    "    #loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels) )\n",
    "    cost1 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=py_x, labels=Y))\n",
    "    print(\"cost1.get_shape() :\", cost1.get_shape())\n",
    "    print(\"regularizer.get_shape() :\", regularizer.get_shape())\n",
    "    cost = (cost1 + beta * regularizer)\n",
    "    print(\"cost.get_shape() :\", cost.get_shape())\n",
    "    \n",
    "#train_op = tf.train.RMSPropOptimizer(0.002, 0.9).minimize(cost)\n",
    "train_op = tf.train.RMSPropOptimizer(0.001, 0.9).minimize(cost)\n",
    "#train_op = tf.train.AdagradOptimizer(0.001).minimize(cost)\n",
    "predict_op = tf.argmax(py_x, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "correct_pred = tf.equal(tf.argmax(Y, 1), tf.argmax(py_x, 1)) # Count correct predictions\n",
    "with tf.name_scope('accuracy'):\n",
    "    acc_op = tf.reduce_mean(tf.cast(correct_pred, \"float\")) # Cast boolean to float to average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# trXP = trXP[:2000]\n",
    "# trXH = trXH[:2000]\n",
    "# trY = trY[:2000]\n",
    "# teXP = teXP[:500]\n",
    "# teXH = teXH[:500]\n",
    "# teY = teY[:500]\n",
    "# teXP1 = teXP1[:500]\n",
    "# teXH1 = teXH1[:500]\n",
    "# teY1 = teY1[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(549367, 42) (549367, 42) (549367, 3) (9842, 42) (9842, 42) (9842, 3)\n",
      "(9824, 42) (9824, 42) (9824, 3)\n"
     ]
    }
   ],
   "source": [
    "print(trXP.shape, trXH.shape, trY.shape, teXP.shape, teXH.shape, teY.shape)\n",
    "print(teXP1.shape, teXH1.shape, teY1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128*2*2\n",
    "dropout_1 = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def eval(XP1, XH1, Y1, batch_size1):\n",
    "    accuracy = 0.0\n",
    "    loss = 0.0\n",
    "    iter_batch = 0.0\n",
    "    for start, end in zip(range(0, len(XP1), batch_size1), range(batch_size1, len(XP1)+1, batch_size1)):\n",
    "        feed_dict = {XP: XP1[start:end], XH: XH1[start:end], Y: Y1[start:end], dropout:1, phase:0}\n",
    "        accuracy_batch, loss_batch = sess.run([acc_op, cost], feed_dict=feed_dict)\n",
    "        accuracy = accuracy + accuracy_batch\n",
    "        loss = loss + loss_batch\n",
    "        iter_batch = iter_batch + 1\n",
    "    accuracy = float(accuracy) / iter_batch\n",
    "    loss = float(loss) / iter_batch\n",
    "    return accuracy, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch0\n",
      "\n",
      "\n",
      "\n",
      "epoch:  0 training: 0.729360590731 0.683081783465\n",
      "epoch:  0 validation: 0.7262852537 0.690789473684\n",
      "epoch:  0 test: 0.721689983418 0.683490953947\n",
      "epoch:  0 328.715721846 113.906031132 2.01069092751 2.01269412041 446.645138025\n",
      "\n",
      "\n",
      "epoch1\n",
      "\n",
      "\n",
      "\n",
      "epoch:  1 training: 0.626383134011 0.739817149604\n",
      "epoch:  1 validation: 0.634005835182 0.736636513158\n",
      "epoch:  1 test: 0.627824253158 0.742084703947\n",
      "epoch:  1 327.665489912 113.44345808 2.01019406319 2.01326489449 445.13240695\n",
      "\n",
      "\n",
      "epoch2\n",
      "\n",
      "\n",
      "\n",
      "epoch:  2 training: 0.569779799473 0.766942266208\n",
      "epoch:  2 validation: 0.581414285459 0.766344572368\n",
      "epoch:  2 test: 0.579275099855 0.768400493421\n",
      "epoch:  2 327.767707109 113.47705698 2.01182389259 2.0151951313 445.271783113\n",
      "\n",
      "\n",
      "epoch3\n",
      "\n",
      "\n",
      "\n",
      "epoch:  3 training: 0.555964714305 0.776523874767\n",
      "epoch:  3 validation: 0.575789112794 0.770970394737\n",
      "epoch:  3 test: 0.574106828163 0.768503289474\n",
      "epoch:  3 327.756392956 113.476868153 2.01390385628 2.01338601112 445.260550976\n",
      "\n",
      "\n",
      "epoch4\n",
      "\n",
      "\n",
      "\n",
      "epoch:  4 training: 0.517676199383 0.796435911264\n",
      "epoch:  4 validation: 0.549029808295 0.788240131579\n",
      "epoch:  4 test: 0.549112087802 0.780633223684\n",
      "epoch:  4 327.881271839 113.478979111 2.01456809044 2.01367092133 445.388489962\n",
      "\n",
      "\n",
      "epoch5\n",
      "\n",
      "\n",
      "\n",
      "epoch:  5 training: 0.488431035722 0.811536191115\n",
      "epoch:  5 validation: 0.529882554945 0.794202302632\n",
      "epoch:  5 test: 0.536062474314 0.789987664474\n",
      "epoch:  5 327.925685167 113.501616955 2.0105099678 2.01193809509 445.449750185\n",
      "\n",
      "\n",
      "epoch6\n",
      "\n",
      "\n",
      "\n",
      "epoch:  6 training: 0.476916329574 0.817903888759\n",
      "epoch:  6 validation: 0.531396436064 0.796566611842\n",
      "epoch:  6 test: 0.535193670737 0.789370888158\n",
      "epoch:  6 327.918408155 113.484397888 2.01075410843 2.01202297211 445.425583124\n",
      "\n",
      "\n",
      "epoch7\n",
      "\n",
      "\n",
      "\n",
      "epoch:  7 training: 0.461761950501 0.825481357859\n",
      "epoch:  7 validation: 0.523767839921 0.794305098684\n",
      "epoch:  7 test: 0.530374401494 0.791837993421\n",
      "epoch:  7 327.900032997 113.472275972 2.01482510567 2.01328897476 445.40042305\n",
      "\n",
      "\n",
      "epoch8\n",
      "\n",
      "\n",
      "\n",
      "epoch:  8 training: 0.439985610044 0.833317543144\n",
      "epoch:  8 validation: 0.520412716426 0.795847039474\n",
      "epoch:  8 test: 0.530014328266 0.792249177632\n",
      "epoch:  8 327.887159109 113.499888897 2.01410102844 2.01106905937 445.412218094\n",
      "\n",
      "\n",
      "epoch9\n",
      "\n",
      "\n",
      "\n",
      "epoch:  9 training: 0.428678824914 0.842598530784\n",
      "epoch:  9 validation: 0.520342687243 0.800575657895\n",
      "epoch:  9 test: 0.530047465312 0.791632401316\n",
      "epoch:  9 327.908061981 113.479995966 2.01372313499 2.01177287102 445.413553953\n",
      "\n",
      "\n",
      "epoch10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    time_init = time.time()\n",
    "    localtime1 = time.asctime( time.localtime(time.time()))\n",
    "    localtime1 = localtime1.replace(\" \", \"_\")\n",
    "    localtime1 = localtime1.replace(\":\", \"_\")\n",
    "    fo = open(\"results_\"+str(localtime1)+\".csv\", \"a\", 0)\n",
    "    fo.write(\"epoch,train_loss,val_loss,test_loss,train_acc,val_acc,test_acc,epoch_time\\n\")\n",
    "    for i in range(45):\n",
    "        time_epoch_start = time.time()\n",
    "        print(\"epoch\" + str(i) + \"\\n\")\n",
    "        for start, end in zip(range(0, len(trXP), batch_size), range(batch_size, len(trXP)+1, batch_size)):\n",
    "            #if start%32768 == 0 and start > 32767:\n",
    "                #print(\"train\", str(i), str(start), str(end))\n",
    "            feed_dict = {XP: trXP[start:end], XH: trXH[start:end], Y: trY[start:end], dropout:dropout_1, phase:1}\n",
    "            sess.run(train_op, feed_dict=feed_dict)\n",
    "            #_, costA, costB, costC = sess.run([train_op, cost1, regularizer, cost], feed_dict=feed_dict)\n",
    "            #print(costA, costB, costC)\n",
    "            #print(\"\\n\")\n",
    "        print(\"\\n\")\n",
    "        train_end = time.time()\n",
    "        accuracy_train, loss_train = eval(trXP, trXH, trY, batch_size)\n",
    "        print(\"epoch: \", i, \"training:\", loss_train, accuracy_train)\n",
    "        train_eval_end = time.time()\n",
    "        accuracy_val, loss_val = eval(teXP, teXH, teY, batch_size)\n",
    "        print(\"epoch: \", i, \"validation:\", loss_val, accuracy_val)\n",
    "        val_eval_end = time.time()\n",
    "        accuracy_test, loss_test = eval(teXP1, teXH1, teY1, batch_size)\n",
    "        print(\"epoch: \", i, \"test:\", loss_test, accuracy_test)\n",
    "        test_eval_end = time.time()\n",
    "        train_time = train_end - time_epoch_start\n",
    "        train_eval_time = train_eval_end - train_end\n",
    "        val_eval_time = val_eval_end - train_eval_end\n",
    "        test_eval_time = test_eval_end - val_eval_end\n",
    "        total_time = test_eval_end - time_epoch_start\n",
    "        print(\"epoch: \", i, train_time, train_eval_time, val_eval_time, test_eval_time, total_time)\n",
    "        print(\"\\n\")\n",
    "        line = str(i) + \",\" + str(loss_train) + \",\" + str(loss_val) + \",\" + str(loss_test) + \",\" + str(accuracy_train) + \",\" + str(accuracy_val) + \",\" + str(accuracy_test) + \",\" + str(total_time) + \"\\n\"\n",
    "        fo.write(line);\n",
    "    all_epoch_end = time.time()\n",
    "    all_epoch_time = all_epoch_end - time_init\n",
    "    print(\"all time taken for epochs :\", all_epoch_time)\n",
    "    fo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
